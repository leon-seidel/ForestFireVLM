{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to gemini-2.0-flash-lite\n"
     ]
    }
   ],
   "source": [
    "from evaluation.google_api import GoogleAPI\n",
    "from evaluation.openai_api import OpenAIAPI\n",
    "from evaluation.forestfire_evaluation import create_predictions, eval_structured_data\n",
    "from templates.answer_schema import smoke_detection_schema\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "# vlm = OpenAIAPI()\n",
    "\n",
    "vlm = GoogleAPI(model_name=\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 4880\n",
      "Model: gemini-2.0-flash-lite\n",
      "\n",
      "\n",
      "Processing batch 1/10 (samples 0-499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:28<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 2/10 (samples 500-999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:55<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 3/10 (samples 1000-1499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:48<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 4/10 (samples 1500-1999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 5/10 (samples 2000-2499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:45<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 6/10 (samples 2500-2999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:44<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 7/10 (samples 3000-3499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:05<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 8/10 (samples 3500-3999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 9/10 (samples 4000-4499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:59<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 500/500 successful\n",
      "\n",
      "Processing batch 10/10 (samples 4500-4879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:34<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete: 380/380 successful\n",
      "\n",
      "Evaluation complete: Successfully processed 4880 out of 4880 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"leon-se/FIgLib-Test\"\n",
    "eval_ds = load_dataset(dataset_name, split=\"train\")\n",
    "print(f\"\\nDataset size: {len(eval_ds)}\\nModel: {vlm.model_name}\\n\")\n",
    "\n",
    "# Create output file (empty it if it exists)\n",
    "output_file = \"benchmarks/batch_eval/gemini-20-flash-lite_figlib.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"\")  # Create empty file\n",
    "\n",
    "# Create a lock for file writing to prevent race conditions\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Process samples in smaller batches\n",
    "BATCH_SIZE = 500\n",
    "MAX_WORKERS = 16  # Reduce the number of concurrent workers\n",
    "\n",
    "def process_sample(sample_idx):\n",
    "    try:\n",
    "        sample = eval_ds[sample_idx]\n",
    "        image = sample[\"image\"]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        gt_dict = sample[\"gt_dict\"]\n",
    "        response_schema = smoke_detection_schema\n",
    "        \n",
    "        # API call to Gemini\n",
    "        vlm_prediction = vlm.generate_structured_response_from_pil_image(prompt, image, response_schema)\n",
    "        \n",
    "        # Thread-safe file writing\n",
    "        with file_lock:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                result = {\"sample_idx\": sample_idx, \"gt_dict\": gt_dict, \"vlm_prediction\": vlm_prediction}\n",
    "                f.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        # Return minimal information\n",
    "        return sample_idx, True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample_idx}: {e}\")\n",
    "        return sample_idx, False\n",
    "\n",
    "# Track successful samples\n",
    "successful_samples = 0\n",
    "total_batches = (len(eval_ds) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Process in batches\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(eval_ds))\n",
    "    print(f\"\\nProcessing batch {batch_idx+1}/{total_batches} (samples {start_idx}-{end_idx-1})\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor with limited workers\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit batch tasks\n",
    "        futures = [executor.submit(process_sample, i) for i in range(start_idx, end_idx)]\n",
    "        \n",
    "        # Process results as they complete with a progress bar\n",
    "        batch_success = 0\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            _, success = future.result()\n",
    "            if success:\n",
    "                batch_success += 1\n",
    "        \n",
    "        successful_samples += batch_success\n",
    "        print(f\"Batch complete: {batch_success}/{end_idx-start_idx} successful\")\n",
    "    \n",
    "    # Force garbage collection between batches\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nEvaluation complete: Successfully processed {successful_samples} out of {len(eval_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "with open(\"benchmarks/batch_eval/gemini-20-flash-lite_figlib.jsonl\", \"r\") as f:\n",
    "    results = [json.loads(line) for line in f.readlines()]\n",
    "    predictions_text = [r[\"vlm_prediction\"] for r in results]\n",
    "    ground_truth_dicts = [r[\"gt_dict\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Results for gemini-2.0-flash-lite on leon-se/FIgLib-Test:\n",
      "\n",
      "forest_fire_smoke_visible: 0.7131\n",
      "overall_score: 0.7131\n",
      "structured_output_correct_ratio: 1.0\n",
      "confusion_forest_fire_smoke_visible: {'forest_fire_smoke_visible': {'accuracy': 0.7131, 'precision': 0.9599, 'recall': 0.4369, 'f1_score': 0.6005}}\n",
      "\n",
      "Results list benchmarks-test/leon-se-FIgLib-Test/eval_results.csv updated\n",
      "Full results saved to benchmarks-test/leon-se-FIgLib-Test/gemini-2.0-flash-lite.pkl\n"
     ]
    }
   ],
   "source": [
    "results = eval_structured_data(predictions_text, ground_truth_dicts, vlm.model_name, dataset_name, \n",
    "                               write_to_file=True, results_folder=\"benchmarks-test\",\n",
    "                               confusion_keys=[\"forest_fire_smoke_visible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.google_api import GoogleAPI\n",
    "from evaluation.openai_api import OpenAIAPI\n",
    "from evaluation.forestfire_evaluation import create_predictions, eval_structured_data\n",
    "from templates.answer_schema import smoke_detection_schema\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "# vlm = OpenAIAPI()\n",
    "\n",
    "vlm = GoogleAPI(model_name=\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"leon-se/FIgLib-Test\"\n",
    "eval_ds = load_dataset(dataset_name, split=\"train\")\n",
    "print(f\"\\nDataset size: {len(eval_ds)}\\nModel: {vlm.model_name}\\n\")\n",
    "\n",
    "# Create output file (empty it if it exists)\n",
    "output_file = \"benchmarks/batch_eval/gemini-20-flash-lite_figlib.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"\")  # Create empty file\n",
    "\n",
    "# Create a lock for file writing to prevent race conditions\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Process samples in smaller batches\n",
    "BATCH_SIZE = 500\n",
    "MAX_WORKERS = 16  # Reduce the number of concurrent workers\n",
    "\n",
    "def process_sample(sample_idx):\n",
    "    try:\n",
    "        sample = eval_ds[sample_idx]\n",
    "        image = sample[\"image\"]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        gt_dict = sample[\"gt_dict\"]\n",
    "        response_schema = smoke_detection_schema\n",
    "        \n",
    "        # API call to Gemini\n",
    "        vlm_prediction = vlm.generate_structured_response_from_pil_image(prompt, image, response_schema)\n",
    "        \n",
    "        # Thread-safe file writing\n",
    "        with file_lock:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                result = {\"sample_idx\": sample_idx, \"gt_dict\": gt_dict, \"vlm_prediction\": vlm_prediction}\n",
    "                f.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        # Return minimal information\n",
    "        return sample_idx, True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample_idx}: {e}\")\n",
    "        return sample_idx, False\n",
    "\n",
    "# Track successful samples\n",
    "successful_samples = 0\n",
    "total_batches = (len(eval_ds) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Process in batches\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(eval_ds))\n",
    "    print(f\"\\nProcessing batch {batch_idx+1}/{total_batches} (samples {start_idx}-{end_idx-1})\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor with limited workers\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit batch tasks\n",
    "        futures = [executor.submit(process_sample, i) for i in range(start_idx, end_idx)]\n",
    "        \n",
    "        # Process results as they complete with a progress bar\n",
    "        batch_success = 0\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            _, success = future.result()\n",
    "            if success:\n",
    "                batch_success += 1\n",
    "        \n",
    "        successful_samples += batch_success\n",
    "        print(f\"Batch complete: {batch_success}/{end_idx-start_idx} successful\")\n",
    "    \n",
    "    # Force garbage collection between batches\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nEvaluation complete: Successfully processed {successful_samples} out of {len(eval_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "with open(\"benchmarks/batch_eval/gemini-20-flash-lite_figlib.jsonl\", \"r\") as f:\n",
    "    results = [json.loads(line) for line in f.readlines()]\n",
    "    predictions_text = [r[\"vlm_prediction\"] for r in results]\n",
    "    ground_truth_dicts = [r[\"gt_dict\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_structured_data(predictions_text, ground_truth_dicts, vlm.model_name, dataset_name, \n",
    "                               write_to_file=True, results_folder=\"benchmarks-test\",\n",
    "                               confusion_keys=[\"forest_fire_smoke_visible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
